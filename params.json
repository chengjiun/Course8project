{"name":"Course8project","tagline":"","body":"\r\n---\r\ntitle: \"DSci-ML Course Project\"\r\noutput: html_document\r\n---\r\n\r\n### Abstract\r\n* Try to use quantified movements to classify the exercises. The data is taken from Velloso et al. 2013. \r\n* The data is cleaned by imputting 0 to NA measurements.\r\n* We model the training set using both CART and gradient boosting tree models. 20% of the training set are left out to examine the accuracy of the model. The models are learned with 10-fold cross-validation, which optimized the kappa. \r\n* The accuracy, sensitivity and specificity of the boosting model reaches above 94%.  \r\n\r\n### Background\r\nThe project is to see\r\nif we can use some quantified movements to predict what motion the testers are doing. The project is based on the research of Ref [1], and their public data. \r\n\r\n```{r include=FALSE}\r\nsetwd('~/Downloads//C8_ML')\r\nlibrary(caret)\r\nlibrary(plyr)\r\nlibrary(rattle)\r\n```\r\n\r\n### A little Data Exploratory\r\n\r\n* Data download: the data is downloaded from \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\". The testing data are from \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\". The training data include the measure of movements from 6 testers, and the motion are classfied into 5 (A,B,C,D,E) catagories. The first catagories A is the real, completed, \"Unilateral Dumbbell Biceps Curl\" motion, and the others are failed motions. The detail can be found in the paper ref[1]. \r\n\r\n```{r include=FALSE,cache=TRUE, message=FALSE, warning=FALSE}\r\nif (!file.exists('pml-training.csv')){\r\n    download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')\r\n}\r\nif (!file.exists('pml-testing.csv')){\r\n    download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')\r\n}\r\ndata = read.csv('pml-training.csv',header=TRUE,stringsAsFactors=FALSE,na.strings=c('NA',''))\r\n# remove index\r\ndata <- data[,!(names(data) %in% c('X'))]\r\n# level the \"classe\" column\r\ndata[,7:158]=lapply( data[,7:158], as.numeric)\r\ndata$classe <- factor(data$classe)\r\ndata$new_window <- factor(data$new_window)\r\ndata$num_window <- factor(data$num_window)\r\n```\r\n\r\n* Examine the training data\r\n```{r, include=FALSE,cache=TRUE, message=FALSE, warning=FALSE}\r\n    names(data)\r\n    summary(data)\r\n    table(data$user_name)\r\n    table(data$classe)\r\n    ggplot(data,aes(x=num_window,y=yaw_belt,color=user_name)) + geom_point() + facet_wrap(~classe)   \r\n```\r\n    1. The first 6 features may contain ID information, so they should be left out as training/testing set. Note that new\\_window probably means if the raw is the last measurement of a window, so that it may be informative. However, for simplicity, they are ignored as well.\r\n    2. Some features are redaudant. For example, there are pairs of var\\_??? and stddev\\_???. I take out the var\\_???.\r\n    3. There are about 2600 ~ 3900 entries for each tester, which should be enough to do 10-fold cross vallidation.\r\n\r\n* Are there any missing data? \r\n```{r ,echo=TRUE, results='hide',fig.show='hide',cache=TRUE, message=FALSE, warning=FALSE}\r\nany(is.na(data))\r\n# woops! many are missing, But, what are they?\r\nNN<-lapply (data, function (x) sum(is.na(x))/length(is.na(x)))\r\nindNA<-which(NN > 0)\r\nNN[indNA]\r\n#There are 100 features with 98% of data missing, why? shall we remove them or add some dummy values?\r\nggplot(data, aes(y=avg_yaw_forearm, x=avg_roll_dumbbell, color=user_name)) + geom_point() + facet_wrap(~classe)\r\nggplot(data,aes(x=roll_belt,color=user_name))+geom_histogram()+facet_wrap(~classe)\r\n# Too complicate!, simply set them the NA value to 0, and keep the feature. \r\n```\r\n    Yes, there are. Almost 98% data of 100 features are missing. No idea why the data are missing, and there is no explanation in the paper. Naively, I will set them to 0 and keep the features. (Imputing them to the mean or other values of the 2% non-NA entries doesn't make sense.)\r\n\r\n* Are there personal differences?\r\n```{r, echo=TRUE, results='hide',cache=TRUE, message=FALSE, warning=FALSE}\r\n  #check if any of the 6 person behaves abnormally\r\n  #plot \r\n  ggplot(data, aes(x=classe))+geom_histogram()+facet_wrap(~user_name)\r\n  # testers didn't do same amount of moves, but shouldn't be a problem\r\n```\r\n```{r, include=TRUE,echo=TRUE, results='hide',fig.show='hide',cache=TRUE, message=FALSE, warning=FALSE}\r\n  # the distribution of each features are different from people, should standardize each feature for each person them\r\nggplot(data,aes(x=max_roll_belt,y=min_roll_belt,color=user_name))+geom_point()+facet_wrap(~classe)\r\n```\r\n    Yes, the distribution of each feature are different among testers. In principle, the data should be standardized by testers. However, it will limits the usage of the model; the model won't work if we apply it to a new tester with few measurements, which is not able to standardized. \r\n    On the other hand, I found the testing data set are from the same testers. This sounds like a leak to me, and I decide to take the advantage to use it. So, the user_name is added back to train the model.\r\n\r\n### Preprocess Data\r\nAccording to the data explortary above, I preprocess the data as following:\r\n```{r, echo=TRUE,cache=TRUE, message=FALSE, warning=FALSE}\r\n# remove id features\r\n  fitdata = data[,7:158]\r\n  fitdata$user_name = data$user_name    \r\n# remove var\r\n  fitdata <- fitdata[,!(names(fitdata) %in% c(\"var_pitch_belt\", \"var_roll_belt\", \"var_yaw_belt\", \"var_roll_arm\", \"var_pitch_arm\", \"var_yaw_arm\", \"var_roll_dumbbell\", \"var_pitch_dumbbell\", \"var_yaw_dumbbell\", \"var_roll_forearm\", \"var_pitch_forearm\",\"var_yaw_forearm\"))]\r\n# standardize (divide the number by the maximum value)\r\n#  fitdata=ddply(fitdata,~user_name,colwise(function(x){x/max(abs(x),na.rm=TRUE)}))\r\n# set NA to 0 \r\n  fitdata[is.na(fitdata)] = 0\r\n fitdata$classe = data$classe\r\n```\r\n\r\n### Experiment\r\nThis is a classification problem, so that the algarithms could be used are like tree, SVD, and etc. Or, we may consider the algarithms combining multiple models, such as random forest, and boosting (gbm in caret). I will try a few different approaches to compare the result, and here is the list: \r\n\r\n* 10-fold cross validation + classification and regression tree (CART; rpart in caret)\r\n\r\n* gradient boosting with trees (gbm in caret)\r\n\r\n* -random forest (rf in caret)- (This is too time consuming, so that it was left out at last.) \r\n\r\nTo securely compare the final result, I keep 20% of data as a testing set. \r\n\r\n```{r echo=TRUE,cache=TRUE, message=FALSE, warning=FALSE}\r\nset.seed(2)\r\ninTrain = createDataPartition(fitdata$classe, p = 0.8, list=FALSE)\r\ntraining = fitdata[ inTrain,]\r\ntesting = fitdata[-inTrain,]\r\n```\r\n\r\n### CART \r\nCART is chosen because of it's interpretibility and simple tunning parameter. It is applied with 10-fold cross validation provided in caret. To be careful, the procedure is repeated for 3 times. In addition, I tried a list of complexity panalty parameter cp=[0.005,0.01,0.03]. \r\n```{r echo=TRUE,cache=TRUE, message=FALSE, warning=FALSE} \r\nset.seed(100)\r\nfitControl <- trainControl(method = \"repeatedcv\",number = 10,repeats=3)\r\nrpartGrid <- expand.grid(cp=c(0.005,0.01,0.03))\r\nMDLcart10f <- train(classe ~ ., data = training, method = \"rpart\",\r\n                 trControl = fitControl, tuneGrid=rpartGrid)\r\nMDLcart10f\r\nfancyRpartPlot(MDLcart10f$finalModel)\r\npredCart10f <- predict(MDLcart10f, newdata=testing)\r\ncmCart10f<-confusionMatrix(predCart10f,testing$classe)\r\ncmCart10f$table\r\n```\r\nThe accuracy looks pretty nice. Specificity and Sensitivity of the testing sample is abouve 80%, which are not bad. But, we should compare to other approaches. \r\n\r\n### Boosting\r\nLet's experiment the gradient boosting tree model (gbm) in caret. I will use the 10-fold cross varification, and repeat = 1. The three default tuning parameters are n.trees, interaction.depth and shrinkage. Since train is going to automatically tune this parameters to find the best model, I will not change the default.\r\n```{r echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE}\r\nset.seed(200)\r\nfitControl <- trainControl(method = \"repeatedcv\",number = 10,repeats=1)\r\nMDLgbm10f <- train(classe ~ ., data = training, method = \"gbm\",trControl = fitControl,verbose=FALSE)\r\nMDLgbm10f\r\npredgbm10f <- predict(MDLgbm10f, newdata=testing)\r\ncmgbm10f<-confusionMatrix(predgbm10f,testing$classe)\r\ncmgbm10f$table\r\n```\r\n\r\n```{r echo=FALSE,cache=TRUE}\r\ncmgbm10f$overall[1:2]\r\ncmgbm10f$byClass[,1:2]\r\n```\r\nHmm, it looks like overfitting? The accuracy, sensitivity, and specificily are all above 94%.  \r\n```{r,echo=FALSE,cache=TRUE}\r\ngbmImp<-varImp(MDLgbm10f)\r\ngbmImp\r\n```\r\nThe list of importance features look normal, and no suspecious ID-related features are found. I will believe the result for now.\r\n\r\n### Summary of Comparison\r\nI use the accuracy, sensitivity, and specificity of the 20% training data set to examine the models. The boosting model seem to provide an almost perfect fit. I can't see why the result is overfitting. If the model is as good as it seems, I would expect the out-of-sample accuracy to be above 95%. \r\n\r\n### Prediction\r\nThe last part is the code to load the real testing data set, and apply the model for prediction. \r\n```{r,cache=TRUE}\r\ntestdata = read.csv('pml-testing.csv',header=TRUE,stringsAsFactors=FALSE,na.strings=c('NA',''))\r\n# remove index\r\ntestdata <- testdata[,!(names(testdata) %in% c('X'))]\r\ntestdata[,7:158]=lapply(testdata[,7:158], as.numeric)\r\n```\r\nHmm, these are the same 6 testers, which means I could use user_name to fit, too. This sounds like a leak to me, though. \r\n```{r echo=FALSE,cache=TRUE}\r\nunique(testdata$user_name)\r\n```\r\n\r\n```{r  message=FALSE, warning=FALSE}\r\n  testdata2 = testdata[,7:158]\r\n  testdata2$user_name=testdata$user_name\r\n  testdata2$problem_id = testdata$problem_id\r\n# remove var\r\n  testdata2 <- testdata2[,!(names(testdata2) %in% c(\"var_pitch_belt\", \"var_roll_belt\", \"var_yaw_belt\", \"var_roll_arm\", \"var_pitch_arm\", \"var_yaw_arm\", \"var_roll_dumbbell\", \"var_pitch_dumbbell\", \"var_yaw_dumbbell\", \"var_roll_forearm\", \"var_pitch_forearm\",\"var_yaw_forearm\"))]\r\n  testdata2$problem_id = testdata$problem_id\r\n# set NA to 0 \r\n  testdata2[is.na(testdata2)] = 0\r\n```\r\nOk! do prediction!\r\n```{r,cache=TRUE}\r\n  pred <- as.character(predict(MDLgbm10f,newdata=testdata2))\r\n```\r\nWrite out the result!\r\n```{r cache=TRUE}\r\n  n = length(pred)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(pred[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n```\r\n\r\n\r\n### Reference:\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. (Read more: http://groupware.les.inf.puc-rio.br/har#ixzz37pRTskR3)\r\nThis dataset is licensed under the Creative Commons license (CC BY-SA).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}